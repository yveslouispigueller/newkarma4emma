"""
Script de Scraping d'Offres d'Emploi RH - Version D√©butant
===========================================================

Ce script permet de r√©cup√©rer automatiquement des offres d'emploi en Ressources Humaines
depuis diff√©rents sites et de les ajouter √† votre page HTML.

IMPORTANT : Le web scraping doit respecter les conditions d'utilisation des sites web.
V√©rifiez toujours les r√®gles de chaque site avant de l'utiliser.
"""

import requests
from bs4 import BeautifulSoup
import json
import time

# ========================================
# CONFIGURATION
# ========================================

# Liste des URLs √† scraper (vous devrez les adapter)
JOBUP_URL = "https://www.jobup.ch/fr/emplois/?term=ressources+humaines+OR+HR+OR+people+officer&location=gen√®ve+OR+vaud+OR+neuch√¢tel"
LINKEDIN_URL = "https://www.linkedin.com/jobs/search/?keywords=HR%20Business%20Partner%20OR%20Chief%20People%20Officer&location=Geneva%2C%20Switzerland"

# D√©lai entre chaque requ√™te (pour √™tre respectueux avec les serveurs)
DELAY_BETWEEN_REQUESTS = 2  # secondes


# ========================================
# FONCTIONS DE SCRAPING
# ========================================

def scrape_jobup(url):
    """
    Fonction pour scraper JobUp.ch
    
    Note pour d√©butants :
    - Cette fonction envoie une requ√™te au site web
    - Elle r√©cup√®re le HTML de la page
    - Elle extrait les informations des offres d'emploi
    """
    jobs = []
    
    try:
        # En-t√™tes pour simuler un navigateur (certains sites les requi√®rent)
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        # Envoi de la requ√™te
        response = requests.get(url, headers=headers, timeout=10)
        
        # V√©rifier si la requ√™te a r√©ussi
        if response.status_code == 200:
            # Parser le HTML
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # IMPORTANT : Vous devez inspecter le site pour trouver les bons s√©lecteurs CSS
            # Ceci est un exemple g√©n√©rique qui devra √™tre adapt√©
            
            job_cards = soup.find_all('div', class_='job-card')  # √Ä ADAPTER
            
            for card in job_cards[:10]:  # Limiter √† 10 offres
                try:
                    job = {
                        'company': card.find('span', class_='company-name').text.strip(),
                        'title': card.find('h3', class_='job-title').text.strip(),
                        'job_title': card.find('span', class_='position').text.strip(),
                        'location': card.find('span', class_='location').text.strip(),
                        'link': card.find('a', class_='job-link')['href']
                    }
                    jobs.append(job)
                except AttributeError:
                    # Si un √©l√©ment n'est pas trouv√©, on passe au suivant
                    continue
        
        else:
            print(f"‚ùå Erreur : Status code {response.status_code}")
    
    except Exception as e:
        print(f"‚ùå Erreur lors du scraping JobUp : {e}")
    
    return jobs


def scrape_linkedin_simple():
    """
    Scraping LinkedIn - VERSION SIMPLIFI√âE
    
    IMPORTANT : LinkedIn est difficile √† scraper directement car :
    1. Il n√©cessite une connexion
    2. Il a des protections anti-scraping
    3. Il utilise beaucoup de JavaScript
    
    Alternative recommand√©e : Utiliser l'API LinkedIn (n√©cessite une cl√© API)
    """
    print("‚ö†Ô∏è  Note : LinkedIn n√©cessite une authentification et est difficile √† scraper.")
    print("   Recommandation : Utilisez l'API LinkedIn officielle ou consultez manuellement.")
    return []


def save_to_html(jobs, output_file='recherche-emploi-emma.html'):
    """
    Fonction pour ajouter les offres au fichier HTML
    
    Cette fonction :
    1. Lit le fichier HTML existant
    2. G√©n√®re le HTML pour les nouvelles offres
    3. Ins√®re les offres dans le tableau
    """
    
    # G√©n√©rer le HTML pour chaque offre
    rows_html = ""
    for job in jobs:
        rows_html += f"""
                    <tr>
                        <td>{job['company']}</td>
                        <td>{job['title']}</td>
                        <td>{job['job_title']}</td>
                        <td>{job['location']}</td>
                        <td><a href="{job['link']}" class="job-link" target="_blank">Voir l'offre</a></td>
                    </tr>
"""
    
    # Lire le fichier HTML existant
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            html_content = f.read()
        
        # Trouver la position o√π ins√©rer les nouvelles offres
        # On cherche la balise de fermeture </tbody>
        insert_position = html_content.find('</tbody>')
        
        if insert_position != -1:
            # Ins√©rer les nouvelles offres avant </tbody>
            new_html = (
                html_content[:insert_position] + 
                rows_html + 
                html_content[insert_position:]
            )
            
            # Sauvegarder le fichier modifi√©
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(new_html)
            
            print(f"‚úÖ {len(jobs)} offres ajout√©es au fichier {output_file}")
        else:
            print("‚ùå Erreur : Impossible de trouver la balise </tbody>")
    
    except FileNotFoundError:
        print(f"‚ùå Erreur : Le fichier {output_file} n'existe pas")
    except Exception as e:
        print(f"‚ùå Erreur lors de la sauvegarde : {e}")


# ========================================
# FONCTION PRINCIPALE
# ========================================

def main():
    """
    Fonction principale qui orchestre tout le processus
    """
    print("üîç D√©but du scraping d'offres d'emploi...")
    print("=" * 50)
    
    all_jobs = []
    
    # Scraper JobUp
    print("\nüìå Scraping JobUp.ch...")
    jobup_jobs = scrape_jobup(JOBUP_URL)
    all_jobs.extend(jobup_jobs)
    print(f"   ‚úì {len(jobup_jobs)} offres trouv√©es")
    
    # Attendre un peu avant la prochaine requ√™te
    time.sleep(DELAY_BETWEEN_REQUESTS)
    
    # Scraper LinkedIn (d√©sactiv√© par d√©faut)
    # linkedin_jobs = scrape_linkedin_simple()
    # all_jobs.extend(linkedin_jobs)
    
    print("\n" + "=" * 50)
    print(f"üìä Total : {len(all_jobs)} offres r√©cup√©r√©es")
    
    # Sauvegarder dans le fichier HTML
    if all_jobs:
        print("\nüíæ Sauvegarde dans le fichier HTML...")
        save_to_html(all_jobs)
    else:
        print("\n‚ö†Ô∏è  Aucune offre √† sauvegarder")
    
    print("\n‚ú® Termin√© !")


# ========================================
# POINT D'ENTR√âE DU SCRIPT
# ========================================

if __name__ == "__main__":
    """
    Cette partie s'ex√©cute quand vous lancez le script directement
    """
    main()


# ========================================
# NOTES POUR ADAPTER LE SCRIPT
# ========================================

"""
COMMENT TROUVER LES BONS S√âLECTEURS CSS :

1. Ouvrez le site web dans votre navigateur (Chrome ou Firefox)
2. Faites un clic droit sur un √©l√©ment que vous voulez r√©cup√©rer
3. Cliquez sur "Inspecter" ou "Examiner l'√©l√©ment"
4. Vous verrez le code HTML de l'√©l√©ment
5. Notez les classes CSS ou les IDs (ex: class="job-title")
6. Utilisez ces informations dans le script :
   - soup.find('div', class_='nom-de-la-classe')
   - soup.find('h3', id='id-element')

EXEMPLE CONCRET :

Si vous voyez dans le HTML :
<div class="job-listing">
    <h2 class="position-title">D√©veloppeur Python</h2>
    <span class="company-name">Google</span>
</div>

Vous pouvez r√©cup√©rer les infos ainsi :
job_card = soup.find('div', class_='job-listing')
title = job_card.find('h2', class_='position-title').text
company = job_card.find('span', class_='company-name').text

CONSEILS :
- Testez d'abord avec un seul site
- V√©rifiez les conditions d'utilisation du site
- Utilisez un d√©lai entre les requ√™tes (time.sleep())
- G√©rez les erreurs avec try/except
"""
